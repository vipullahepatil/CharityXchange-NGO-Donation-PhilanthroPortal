<?xml version="1.0" encoding="UTF-8"?>
<feed xmlns="http://www.w3.org/2005/Atom" xmlns:dc="http://purl.org/dc/elements/1.1/"><title>JBoss Tools Aggregated Feed</title><link rel="alternate" href="http://tools.jboss.org" /><subtitle>JBoss Tools Aggregated Feed</subtitle><dc:creator>JBoss Tools</dc:creator><entry><title>A tutorial on Middleware Automation Collections</title><link rel="alternate" href="https://developers.redhat.com/articles/2023/03/14/tutorial-middleware-automation-collections" /><author><name>Harsha Cherukuri</name></author><id>3bba2083-ef20-4dd8-911f-81088d04cc6d</id><updated>2023-03-14T07:00:00Z</updated><published>2023-03-14T07:00:00Z</published><summary type="html">&lt;p&gt;Getting up to speed with &lt;a href="https://github.com/ansible-middleware"&gt;Ansible Middleware&lt;/a&gt; Collections &lt;span&gt;is easy, and installing the Red Hat Ansible Automation Platform only requires a few steps. This tutorial demonstrates &lt;/span&gt;six steps to configure a Wildfly instance using Ansible by&lt;span&gt; preparing a local machine with the necessary tooling and then deploying an instance of Wildfly using the Wildfly collection provided by the &lt;/span&gt;&lt;a href="https://github.com/ansible-middleware"&gt;Ansible Middleware&lt;/a&gt;.&lt;/p&gt; &lt;h2&gt;Step 1: Install Ansible Automation Platform&lt;/h2&gt; &lt;p&gt;First, let’s get Ansible &lt;span&gt;Automation Platform &lt;/span&gt;installed on the control node. A control node is a machine from which we will push the configurations to the managed nodes/hosts. Managed nodes are the ones we would like to configure and they can be defined under inventory. We can install Ansible &lt;span&gt;Automation Platform&lt;/span&gt; using your preferred method. You can refer to the &lt;a href="https://docs.ansible.com/ansible/latest/installation_guide/intro_installation.html#installation-guide"&gt;installing Ansible&lt;/a&gt; documentation.&lt;/p&gt; &lt;h2&gt;Step 2: Install galaxy server&lt;/h2&gt; &lt;p&gt;Ansible Content Collections is a distribution format for content that can include playbooks, roles, modules, and plugins. By default, while installing collections using ansible-galaxy collection, we are referring to the &lt;a href="https://galaxy.ansible.com"&gt;&lt;u&gt;Galaxy server&lt;/u&gt;&lt;/a&gt;. But we can configure a different galaxy server like the Ansible automation hub by providing the details of the server on ansible.cfg. You can follow the &lt;a href="https://docs.ansible.com/ansible/latest/galaxy/user_guide.html#downloading-a-collection-from-automation-hub"&gt;&lt;u&gt;guide&lt;/u&gt;&lt;/a&gt; to do so. In this tutorial, we would be using the &lt;a href="https://galaxy.ansible.com"&gt;&lt;u&gt;Galaxy server&lt;/u&gt;&lt;/a&gt; to install and configure using the &lt;a href="https://galaxy.ansible.com/middleware_automation/wildfly"&gt;&lt;u&gt;middleware_automation.wildfly&lt;/u&gt;&lt;/a&gt; collection.&lt;/p&gt; &lt;h2&gt;Step 3: Installing the ansible-navigator utility&lt;/h2&gt; &lt;p&gt;We will use an &lt;a href="https://docs.ansible.com/automation-controller/latest/html/userguide/execution_environments.html"&gt;execution environment&lt;/a&gt; and the &lt;a href="https://ansible-navigator.readthedocs.io/en/latest/installation/#linux"&gt;&lt;u&gt;ansible-navigator&lt;/u&gt;&lt;/a&gt; utility to perform the automation and provisioning of the Wildfly instance. Install and configure ansible-navigator based on the documentation for your target operating system. To perform the execution of the automation, the &lt;a href="https://quay.io/repository/ansible-middleware/ansible-middleware-ee"&gt;ansible-middleware-ee&lt;/a&gt; execution environment provided by the team includes all of the Ansible Content Collections &lt;span&gt;and their dependencies. So there is no need to install any additional components on the control node. We would be using our execution environment as it already includes all of our latest collections so we don’t have to set it up again.&lt;/span&gt;&lt;/p&gt; &lt;p&gt;Once ansible-navigator has been installed, execute the following command to browse all of the collections that are included within the execution environment image:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;$ ansible-navigator --eei quay.io/ansible-middleware/ansible-middleware-ee:latest collections&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;In the following output, we see the list of all the collections available in our execution environment.&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt; Name Version Shadowed Type Path 0│ansible.builtin 2.12.5.post0 False contained /usr/local/lib/python3.8/site-packages/ansible 1│ansible.netcommon 3.0.0 False contained /usr/share/ansible/collections/ansible_collections/ansible 2│ansible.posix 1.4.0 False contained /usr/share/ansible/collections/ansible_collections/ansible 3│ansible.utils 2.6.1 False contained /usr/share/ansible/collections/ansible_collections/ansible 4│community.general 5.0.0 False contained /usr/share/ansible/collections/ansible_collections/communi 5│middleware_automation.amq 0.0.2 False contained /usr/share/ansible/collections/ansible_collections/middlew 6│middleware_automation.infinispan 1.0.3 False contained /usr/share/ansible/collections/ansible_collections/middlew 7│middleware_automation.jcliff 0.0.21 False contained /usr/share/ansible/collections/ansible_collections/middlew 8│middleware_automation.jws 0.0.3 False contained /usr/share/ansible/collections/ansible_collections/middlew 9│middleware_automation.keycloak 1.0.4 False contained /usr/share/ansible/collections/ansible_collections/middlew 10│middleware_automation.redhat_csp_download 1.2.2 False contained /usr/share/ansible/collections/ansible_collections/middlew 11│middleware_automation.wildfly 1.0.2 False contained /usr/share/ansible/collections/ansible_collections/middlew &lt;/code&gt;&lt;/pre&gt; &lt;p class="text-align-center"&gt; &lt;/p&gt; &lt;h2&gt;Step 4: Setting up the inventory&lt;/h2&gt; &lt;p&gt;Let's now set up a Wildfly instance. Create an inventory file that includes a Red Hat Enterprise Linux 8 instance, the IP address of the instance, and login information for ansible to access it. We are using SSH keys instead of passwords. So these SSH keys are created on the controller node and we provide the path of the private key in the inventory file. Our inventory file looks like this:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;[wildfly] wildfly-0 ansible_host=10.0.148.43 ansible_user=root ansible_ssh_private_key_file=”path to your private key” &lt;/code&gt;&lt;/pre&gt; &lt;h2&gt;Step 5: Installing and configuring Wildfly&lt;/h2&gt; &lt;p&gt;Here is our Ansible Playbook wildfly.yml for installing and configuring Wildfly:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-yaml"&gt;--- - name: "Installation and configuration" hosts: wildfly vars: wildfly_install_workdir: '/opt' install_name: "wildfly" wildfly_user: "{{ install_name }}" wildfly_config_base: standalone-ha.xml wildfly_version: "26.0.0.Final" wildfly_home: "{{ wildfly_install_workdir }}/{{ install_name }}-{{ wildfly_version }}" collections: - middleware_automation.wildfly tasks: - name: Include wildfly role ansible.builtin.include_role: name: middleware_automation.wildfly.wildfly_install - name: "Set up for Wildfly instance" include_role: name: middleware_automation.wildfly.wildfly_systemd vars: wildfly_config_base: 'standalone-ha.xml' wildfly_basedir_prefix: "/opt/{{ install_name }}" wildfly_config_name: "{{ install_name }}" wildfly_instance_name: "{{ install_name }}" service_systemd_env_file: "/etc/{{ install_name }}.conf" service_systemd_conf_file: "/usr/lib/systemd/system/{{ install_name }}.service" &lt;/code&gt;&lt;/pre&gt; &lt;h2&gt;Step 6: Run the Ansible Playbook&lt;/h2&gt; &lt;p&gt;Now, run the Ansible Playbook using ansible-navigator and the execution environment to configure wildfly on the remote node as follows:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;$ ansible-navigator --eei quay.io/ansible-middleware/ansible-middleware-ee:latest run wildfly.yml -i inventory -m stdout --become&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Once the playbook has completed executing, ssh into the instance and check the status and health check of the deployed wildfly service. We can do so with the following command:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;$ ssh root@10.0.148.43 curl http://127.0.0.1:9990/health &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;The following code snippet shows the output:&lt;/p&gt; &lt;pre&gt; &lt;code&gt;[{"name" : "boot-errors", "outcome" : true},{"name" : "deployments-status", "outcome" : true},{"name" : "server-state", "outcome" : true, "data" : [{ "value" : "running" }]},{"name" : "live-server", "outcome" : true},{"name" : "started-server", "outcome" : true},{ "outcome" : true }]&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Check the status of the Wildfly service using the following command:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;$ ssh root@10.0.148.43 systemctl status wildfly &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;In the following output, we can see the Wildfly service is running without any errors:&lt;/p&gt; &lt;pre&gt; &lt;code&gt;● wildfly.service - JBoss EAP (standalone mode) Loaded: loaded (/usr/lib/systemd/system/wildfly.service; enabled; vendor preset: disabled) Active: active (running) since Thu 2023-03-02 18:02:44 EST; 4 days ago Main PID: 52667 (standalone.sh) Tasks: 45 (limit: 23417) Memory: 263.6M CGroup: /system.slice/wildfly.service ├─52667 /bin/sh /opt/wildfly-26.0.0.Final/bin/standalone.sh -c wildfly.xml -b 0.0.0.0 -Djboss.server.config.dir=/opt/wildfly&gt; └─52741 java -D[Standalone] -server -Xms64m -Xmx512m -XX:MetaspaceSize=96M -XX:MaxMetaspaceSize=256m -Djava.net.preferIPv4St&gt; Mar 02 18:02:47 wildfly-0 standalone.sh[52741]: 18:02:47,802 INFO [org.jboss.modcluster] (ServerService Thread Pool -- 84) MODCLUSTER0&gt; Mar 02 18:02:47 wildfly-0 standalone.sh[52741]: 18:02:47,829 INFO [org.wildfly.extension.undertow] (MSC service thread 1-1) WFLYUT0006&gt; Mar 02 18:02:47 wildfly-0 standalone.sh[52741]: 18:02:47,864 INFO [org.jboss.as.connector.subsystems.datasources] (MSC service thread &gt; Mar 02 18:02:47 wildfly-0 standalone.sh[52741]: 18:02:47,953 INFO [org.jboss.as.patching] (MSC service thread 1-2) WFLYPAT0050: WildFl&gt; Mar 02 18:02:47 wildfly-0 standalone.sh[52741]: 18:02:47,988 INFO [org.jboss.as.server.deployment.scanner] (MSC service thread 1-1) WF&gt; Mar 02 18:02:48 wildfly-0 standalone.sh[52741]: 18:02:48,002 INFO [org.jboss.ws.common.management] (MSC service thread 1-3) JBWS022052&gt; Mar 02 18:02:48 wildfly-0 standalone.sh[52741]: 18:02:48,179 INFO [org.jboss.as.server] (Controller Boot Thread) WFLYSRV0212: Resuming&gt; Mar 02 18:02:48 wildfly-0 standalone.sh[52741]: 18:02:48,182 INFO [org.jboss.as] (Controller Boot Thread) WFLYSRV0025: WildFly Full 26&gt; Mar 02 18:02:48 wildfly-0 standalone.sh[52741]: 18:02:48,183 INFO [org.jboss.as] (Controller Boot Thread) WFLYSRV0060: Http management&gt; Mar 02 18:02:48 wildfly-0 standalone.sh[52741]: 18:02:48,183 INFO [org.jboss.as] (Controller Boot Thread) WFLYSRV0051: Admin console l&gt; lines 1-20/20 (END)&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;We can also validate the instance using our &lt;a href="https://github.com/ansible-middleware/wildfly-cluster-demo/blob/main/validate.yml"&gt;&lt;u&gt;validate.yml&lt;/u&gt;&lt;/a&gt;. This will check if the instance is running and if the Wildfly port is accessible for use.&lt;/p&gt; &lt;h2&gt;Explore other tutorials and resources&lt;/h2&gt; &lt;p&gt;In this tutorial, we demonstrated how to set up and provision an instance of Wildfly using the Ansible Content Collections &lt;span&gt;for Wildfly. We can also deploy JBoss EAP instead of the open-source Wildfly using the same collection. For more information on deploying JBoss EAP refer to &lt;/span&gt;&lt;a href="https://developers.redhat.com/articles/2022/02/08/automate-and-deploy-jboss-eap-cluster-ansible#"&gt;&lt;u&gt;Automate and deploy a JBoss EAP cluster with Ansible&lt;/u&gt;&lt;/a&gt;&lt;span&gt;. To deploy Wildfly or JBoss EAP on multiple instances, you can refer to our &lt;/span&gt;&lt;a href="https://github.com/ansible-middleware/wildfly-cluster-demo"&gt;&lt;u&gt;wildfly-cluster-demo&lt;/u&gt;&lt;/a&gt;&lt;span&gt;. &lt;/span&gt;&lt;/p&gt; &lt;p&gt;&lt;span&gt;For a more complex scenario, check out our &lt;/span&gt;&lt;a href="https://github.com/ansible-middleware/flange-demo"&gt;&lt;u&gt;Flange project demo&lt;/u&gt;&lt;/a&gt;&lt;span&gt;, which uses the JBoss EAP, &lt;/span&gt;&lt;a href="https://access.redhat.com/products/red-hat-single-sign-on"&gt;&lt;u&gt;Red Hat Single Sign-On&lt;/u&gt;&lt;/a&gt;&lt;span&gt;, and &lt;/span&gt;&lt;a href="https://developers.redhat.com/products/datagrid/overview"&gt;&lt;u&gt;Red Hat Data Grid&lt;/u&gt;&lt;/a&gt;&lt;span&gt; as a cache and &lt;/span&gt;&lt;a href="https://access.redhat.com/collections/red-hat-jboss-core-services-collection"&gt;&lt;u&gt;Red Hat Middleware Core Services Collection&lt;/u&gt;&lt;/a&gt;&lt;span&gt; as a load balancer. Also, our &lt;/span&gt;&lt;a href="https://console.redhat.com/ansible/automation-hub/repo/published/redhat/jboss_eap/"&gt;&lt;u&gt;redhat.jboss_eap&lt;/u&gt;&lt;/a&gt;&lt;span&gt; collection is available on Ansible automation hub. Note that this collection is a Beta release and for &lt;/span&gt;&lt;a href="https://access.redhat.com/support/offerings/techpreview"&gt;&lt;u&gt;Technical Preview&lt;/u&gt;&lt;/a&gt;&lt;span&gt;.&lt;/span&gt;&lt;/p&gt; &lt;p&gt;You can check out the other collections and demos within the GitHub organization: &lt;a href="https://github.com/ansible-middleware"&gt;&lt;u&gt;ansible-middleware&lt;/u&gt;&lt;/a&gt; and the &lt;a href="https://ansiblemiddleware.com"&gt;Middleware Automation Collections&lt;/a&gt; &lt;span&gt;website.&lt;/span&gt;&lt;/p&gt; The post &lt;a href="https://developers.redhat.com/articles/2023/03/14/tutorial-middleware-automation-collections" title="A tutorial on Middleware Automation Collections"&gt;A tutorial on Middleware Automation Collections&lt;/a&gt; appeared first on &lt;a href="https://developers.redhat.com/blog" title="Red Hat Developer"&gt;Red Hat Developer&lt;/a&gt;. &lt;br /&gt;&lt;br /&gt;</summary><dc:creator>Harsha Cherukuri</dc:creator><dc:date>2023-03-14T07:00:00Z</dc:date></entry><entry><title type="html">Serverless Workflow VSCode Editor support to Open API and Async API</title><link rel="alternate" href="https://blog.kie.org/2023/03/serverless-workflow-vscode-editor-support-to-open-api-and-async-api.html" /><author><name>Saravana Balaji</name></author><id>https://blog.kie.org/2023/03/serverless-workflow-vscode-editor-support-to-open-api-and-async-api.html</id><updated>2023-03-13T20:08:08Z</updated><content type="html">In the previous we saw how one can register OpenAPI endpoints and AsyncAPI channels in Service registry and pass them as functions. Also we saw steps to use those functions in Serverless Logic Web Tools via Service Catalog Explorer’s autocomplete feature. This article explains how this same feature works in Kogito Serverless Workflow VS Code extension. In this extension, the API specifications can be uploaded via external registries, or you can create a folder named specs within the project and place the specification files there. You can look at the repository for examples of workflows and specification files. SERVICE CATALOG IN SERVERLESS WORKFLOW VSCODE EXTENSION To proceed further, you will need and the Kogito Serverless Workflow VS Code extension. There are several ways to download and install Kogito Serverless Workflow VS Code extension 1. You can download from the 2. Click on the Extensions icon in the Activity Bar on the side of VS Code, search and install. * 3. Launch VS Code Quick Open (Ctrl+P), paste the following commands, and press enter: * ext install redhat.vscode-extension-serverless-workflow-editor Before proceeding with the project, let us set up the service registries. Some of the features in the Serverless Workflow Editor require integration with Red Hat OpenShift Application and Data Services. Here, uploading API specifications to a service registry requires this integration. Now, in the Red Hat OpenShift application and Data Services, let us create a service registry. CREATING A SERVICE REGISTRY You can create or use a Service Registry instance from your Red Hat OpenShift Application and Data Services console and add the Service Registry to Serverless Logic Web Tools. Prerequisites * You have access to the Red Hat OpenShift Application and Data Services console. Procedure 1. To create a Service Registry instance in the Red Hat Openshift Application and Data Services console, perform the following steps: 1. Go to the. 2. Click the “Create Service Registry Instance” button. 2. Enter a Service Registry instance name in the Create a Service Registry Instance window and click Create. Click on the menu in the top-right corner of the screen. 3. Click Connection. 1. A drawer opens, containing the required connection and authentication information. 2. Copy the value of the Core Registry API. 4. If you already have a Service Registry, find the value of the Core Registry API of your Service Registry. UPLOAD THE API SPECIFICATION: Once you have the Service registry created, you are set to upload the API specification file, which could be an Open API or an Async API. Procedure: 1. On the OpenShift Application Console, go to the Service Registry instance that you created. 2. Click the Upload Artifact button, and a modal window opens. 3. On the modal, enter the group and artifacts ID, which are mandatory fields. 4. You can choose type from the dropdown menu, or you can leave the default option Auto-Detect alone. 5. In the artifact section, you can browse or drag and drop the API specification file, or you can also copy-paste the content directly into the text area. 6. Finally, click “upload.” CONFIGURE YOUR VISUAL STUDIO CODE Open the Command Palette: * Hit F1 or select from the menu View &gt; Command Palette… Type Pref and select Preferences: Open User Settings (JSON) Add the following configuration key and close it: "kogito.swf.serviceRegistries": { "registries": [ { "authProvider": "red-hat-account", "name": &lt;Provide your registry name&gt;, "url": &lt;Paste Core Registry API copied from console&gt; } ] } Against the property authProvider you can provide either none or ‘red-hat-account’. If you choose ‘red-hat-account’, then you will need to install and provide authentication, steps are as follows. ACCESSING THE SERVICE CATALOG IN VSCODE EXTENSION When you have all the prerequisites ready, it is possible to create a workflow and try the Service Catalog feature. You can create an empty file with a filename and extension ‘.sw.json’ or ‘.sw.yaml’. Open the created document. You can click on the Create a Serverless Workflow option to generate a basic template workflow. Just above the functions property, there will be a Setup Service Registries option. When you click, it will take you through the Red Hat account login screen where you can enter your credentials. Post login you can click on the Refresh Service Registries option to refresh the editor. You will find the Add Function option clicking on it, a side widget containing a list of function suggestions will appear. These functions are extracted from the API specification files that you uploaded on the Red Hat OpenShift Application Console. When you select a particular function, it will automatically generate auto-filled name, operation, and type properties. With this we have seen how you can register OpenAPI endpoints and AsyncAPI channels through Service registries and pass them as functions to the editor. Thats all for now, Stay tuned for more exciting new features on Kogito Serverless Workflow editors. The post appeared first on .</content><dc:creator>Saravana Balaji</dc:creator></entry><entry><title>How to use automation controller to install MS SQL</title><link rel="alternate" href="https://developers.redhat.com/articles/2023/03/13/how-use-automation-controller-install-ms-sql" /><author><name>Nagesh Rathod</name></author><id>2d761366-b417-4710-bdf3-b24bd48a6b92</id><updated>2023-03-13T07:00:00Z</updated><published>2023-03-13T07:00:00Z</published><summary type="html">&lt;p&gt;The purpose of this article is to demonstrate how to create an execution environment with custom dependencies and how to execute Ansible Playbooks using the automation controller's GUI, a component of Red Hat Ansible Automation Platform. For this article, we will use Ansible Roles to install Microsoft SQL on Red Hat Enterprise Linux 8.&lt;/p&gt; &lt;p&gt;Make sure you have the &lt;a href="https://developers.redhat.com/products/ansible/overview"&gt;&lt;u&gt;Ansible Automation Platform&lt;/u&gt;&lt;/a&gt; installed on your machine before you begin. For more information about Ansible Automation Platform installation, please refer to our previous article, &lt;a href="https://developers.redhat.com/articles/2023/01/01/how-install-red-hat-ansible-automation-platform-rhel-9#"&gt;How to install Red Hat Ansible Automation Platform on RHEL 9&lt;/a&gt;. Follow these five steps to complete this demonstration:&lt;/p&gt; &lt;h2&gt;Step 1. Setting up the automation execution environment&lt;/h2&gt; &lt;p&gt;Automation execution environments provide a defined, consistent, and portable environment for executing automation jobs. Unlike legacy virtual environments, automation execution environments are Linux container images that make it possible to incorporate system-level dependencies and collection-based content. Each automation execution environment allows you to have a customized image to run jobs, and each of them contains only what you need when running the job.&lt;/p&gt; &lt;p&gt;There are dependencies for the automation execution environment, such as Python 3 and Podman. Make sure these tools are installed. We have provided instructions for installing and using Podman in this &lt;a href="https://developers.redhat.com/videos/youtube/bJDI_QuXeCE"&gt;video&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;Before you can complete any of the following tasks, you must create a registry &lt;a href="https://access.redhat.com/terms-based-registry/"&gt;&lt;u&gt;service account&lt;/u&gt;&lt;/a&gt;. To log in, open up your terminal and type the following commands:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;podman login registry.redhat.io Username: {REGISTRY-SERVICE-ACCOUNT-USERNAME} Password: {REGISTRY-SERVICE-ACCOUNT-PASSWORD} Login Succeeded!&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Once we are successfully logged in, we need to create a container image by using a &lt;strong&gt;Containerfile&lt;/strong&gt; containing the following context:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;​​​​​​​FROM registry.redhat.io/ansible-automation-platform-22/ee-29-rhel8:latest RUN ansible-galaxy collection install microsoft.sql&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;To build an image using podman enter the following:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;​​​​​​​podman build -t &lt;image-name&gt;.&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;The image should be pushed into the container image registry. Log in to the private container image registry using the command 'podman login' before pushing.&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;podman push &lt;image-name&gt;&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Add the image name in the automation execution environment, as shown in Figure 1.&lt;/p&gt; &lt;figure class="align-center" role="group"&gt;&lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/1_0.png" data-featherlight="image"&gt;&lt;img loading="lazy" src="https://developers.redhat.com/sites/default/files/styles/article_floated/public/1_0.png?itok=mgnWbc_Q" width="600" height="293" alt="The edit details section of the Execution Environment page in Ansible Automation Platform." typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt;&lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt;Figure 1: The Execution Environment page.&lt;/div&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt; &lt;/p&gt; &lt;h2&gt;Step​​​​​​​ 2. Set up the inventory&lt;/h2&gt; &lt;p&gt;An inventory is a collection of hosts against which jobs may be launched. To create inventory in Ansible Automation Platform please follow these steps and refer to Figure 2:&lt;/p&gt; &lt;ol&gt;&lt;li aria-level="1"&gt;Select the inventory from the left menu.&lt;/li&gt; &lt;li aria-level="1"&gt;Click on &lt;strong&gt;add&lt;/strong&gt;.&lt;/li&gt; &lt;li aria-level="1"&gt;Select &lt;strong&gt;add inventory&lt;/strong&gt;.&lt;/li&gt; &lt;li aria-level="1"&gt;Give a name to the inventory and save it.&lt;/li&gt; &lt;li aria-level="1"&gt;Select the hosts from inventories and click &lt;strong&gt;add hosts&lt;/strong&gt;.&lt;/li&gt; &lt;li aria-level="1"&gt;Give the targeted server IP or name and save it.&lt;/li&gt; &lt;/ol&gt;&lt;figure class="align-center" role="group"&gt;&lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/2_4.png" data-featherlight="image"&gt;&lt;img loading="lazy" src="https://developers.redhat.com/sites/default/files/styles/article_floated/public/2_4.png?itok=mWv_pWQN" width="600" height="217" alt="The Inventory page in Ansible Automation Platform." typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt;&lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt;Figure 2: The Inventory page.&lt;/div&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt; &lt;/p&gt; &lt;h2&gt;Step 3. Set up the credentials&lt;/h2&gt; &lt;p&gt;To connect with the target server, we need credentials such as username, password, or ssh key. By using credentials, we can pass the required credentials during the playbook execution.&lt;/p&gt; &lt;p&gt;Follow these steps and refer to Figure 3:&lt;/p&gt; &lt;ol&gt;&lt;li aria-level="1"&gt;Select the credentials from the left menu.&lt;/li&gt; &lt;li aria-level="1"&gt;Click on new credentials and select Machine credentials type.&lt;/li&gt; &lt;li aria-level="1"&gt;Add your username, password, or ssh key in the corresponding fields.&lt;/li&gt; &lt;/ol&gt;&lt;figure class="align-center" role="group"&gt;&lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/3_6.png" data-featherlight="image"&gt;&lt;img loading="lazy" src="https://developers.redhat.com/sites/default/files/styles/article_floated/public/3_6.png?itok=QJ4O8qnN" width="600" height="219" alt="The credentials page in Ansible Automation Platform." typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt;&lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt;Figure 3: The credentials page.&lt;/div&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt; &lt;/p&gt; &lt;h2&gt;Step 4. Configuring a project&lt;/h2&gt; &lt;p&gt;A project is a logical collection of Ansible Playbooks represented in the controller. You can manage playbooks and playbook directories on your controller server either manually or by using a source code management (SCM) system such as Git, Subversion, or Mercurial supported by the controller.&lt;/p&gt; &lt;p&gt;Follow these steps to create a project and refer to Figure 4:&lt;/p&gt; &lt;ol&gt;&lt;li aria-level="1"&gt;Create a new project for our git repository from the left menu.&lt;/li&gt; &lt;li aria-level="1"&gt;Click on the &lt;strong&gt;+&lt;/strong&gt; icon in the right corner.&lt;/li&gt; &lt;li aria-level="1"&gt;Give the project a name.&lt;/li&gt; &lt;li aria-level="1"&gt;Select your organization (or choose &lt;strong&gt;Default&lt;/strong&gt;).&lt;/li&gt; &lt;li aria-level="1"&gt;Select the SCM TYPE (GIT in our case).&lt;/li&gt; &lt;li aria-level="1"&gt;Add RESOURCE DETAILS &lt;ul&gt;&lt;li aria-level="2"&gt;SCM &lt;a href="https://github.com/redhat-developer-demos/MicrosoftSQL-AAP-on-RHEL"&gt;&lt;u&gt;URL&lt;/u&gt;&lt;/a&gt;&lt;/li&gt; &lt;li aria-level="2"&gt;SCM BRANCH(main)&lt;/li&gt; &lt;li aria-level="2"&gt;SCM CREDENTIAL&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li aria-level="2"&gt;Save it.&lt;/li&gt; &lt;/ol&gt;&lt;figure class="align-center" role="group"&gt;&lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/4_3.png" data-featherlight="image"&gt;&lt;img loading="lazy" src="https://developers.redhat.com/sites/default/files/styles/article_floated/public/4_3.png?itok=RGZpqYlc" width="600" height="298" alt="The Project page of Ansible Automation Platform." typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt;&lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt;Figure 4: The Project page.&lt;/div&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt; &lt;/p&gt; &lt;h2&gt;Step 5. Configuring templates&lt;/h2&gt; &lt;p&gt;&lt;a href="https://docs.ansible.com/automation-controller/latest/html/userguide/glossary.html#term-Job-Template"&gt;&lt;u&gt;Templates&lt;/u&gt;&lt;/a&gt; define and set parameters for running jobs. A template is more like a blueprint where all of the dependencies are defined, such as inventory, projects, credentials, etc.&lt;/p&gt; &lt;p&gt;Follow these steps to create a template to execute the job for us (see Figure 5):&lt;/p&gt; &lt;ol&gt;&lt;li aria-level="1"&gt;From the left menu, select templates and create a new template.&lt;/li&gt; &lt;li aria-level="1"&gt;Click on + icon from the right corner and select the Job template.&lt;/li&gt; &lt;li aria-level="1"&gt;Give the template a name.&lt;/li&gt; &lt;li aria-level="1"&gt;Select the project and playbook you want to run in the template.&lt;/li&gt; &lt;li aria-level="1"&gt;Select &lt;a href="https://github.com/redhat-developer-demos/MicrosoftSQL-AAP-on-RHEL/blob/main/microsoft_sql_playbook.yaml"&gt;microsoft_sql_playbook.yaml&lt;/a&gt; playbook.&lt;/li&gt; &lt;li aria-level="1"&gt;Select the execution environment which you created previously.&lt;/li&gt; &lt;/ol&gt;&lt;pre&gt; &lt;code class="language-bash"&gt;--- - hosts: dev   collections:   - microsoft.sql   vars:     mssql_accept_microsoft_odbc_driver_17_for_sql_server_eula: true     mssql_accept_microsoft_cli_utilities_for_sql_server_eula: true     mssql_accept_microsoft_sql_server_standard_eula: true     mssql_password: "YourP@ssw0rd"     mssql_edition: Evaluation     mssql_enable_sql_agent: true     mssql_install_fts: true     mssql_install_powershell: true     mssql_tune_for_fua_storage: true   roles:     - microsoft.sql.server​&lt;/code&gt;&lt;/pre&gt; &lt;figure class="align-center" role="group"&gt;&lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/5_5.png" data-featherlight="image"&gt;&lt;img loading="lazy" src="https://developers.redhat.com/sites/default/files/styles/article_floated/public/5_5.png?itok=uRgIOScv" width="600" height="297" alt="The templates page in Ansible Automation Platform." typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt;&lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt;Figure 5: The Templates page.&lt;/div&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;&lt;/figcaption&gt;&lt;/figure&gt;&lt;p class="Indent1"&gt;7. ​​​​​​​Launch it (Figure 6).&lt;/p&gt; &lt;figure class="align-center" role="group"&gt;&lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/6_2.jpg" data-featherlight="image"&gt;&lt;img loading="lazy" src="https://developers.redhat.com/sites/default/files/styles/article_floated/public/6_2.jpg?itok=5dhT1dcv" width="600" height="298" alt="After launching, this page shows the output of a successful installation of MicrosoftSQL server." typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt;&lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt;Figure 6: After launching, a successful installation of MicrosoftSQL server.&lt;/div&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt; &lt;/p&gt; &lt;h2&gt;Continue your automation journey with Ansible&lt;/h2&gt; &lt;p&gt;&lt;a href="https://developers.redhat.com/products/ansible/getting-started"&gt;&lt;u&gt;Get started&lt;/u&gt;&lt;/a&gt; with the Ansible Automation Platform by exploring &lt;a href="https://developers.redhat.com/products/ansible/getting-started"&gt;interactive labs&lt;/a&gt;. Ansible Automation Platform is also available as a managed offering on&lt;a href="https://www.redhat.com/en/technologies/management/ansible/azure"&gt;&lt;u&gt; Microsoft Azure&lt;/u&gt;&lt;/a&gt; and as a self-managed offering on &lt;a href="https://www.redhat.com/en/technologies/management/ansible/aws"&gt;&lt;u&gt;AWS&lt;/u&gt;&lt;/a&gt;.&lt;/p&gt; The post &lt;a href="https://developers.redhat.com/articles/2023/03/13/how-use-automation-controller-install-ms-sql" title="How to use automation controller to install MS SQL"&gt;How to use automation controller to install MS SQL&lt;/a&gt; appeared first on &lt;a href="https://developers.redhat.com/blog" title="Red Hat Developer"&gt;Red Hat Developer&lt;/a&gt;. &lt;br /&gt;&lt;br /&gt;</summary><dc:creator>Nagesh Rathod</dc:creator><dc:date>2023-03-13T07:00:00Z</dc:date></entry><entry><title type="html">Infinispan 14.0.7.Final</title><link rel="alternate" href="https://infinispan.org/blog/2023/03/13/infinispan-14" /><author><name>Tristan Tarrant</name></author><id>https://infinispan.org/blog/2023/03/13/infinispan-14</id><updated>2023-03-13T00:00:00Z</updated><content type="html">We rarely do announcements for micro-releases, but 14.0.7.Final is a bit special, because it finally adds support for Spring 6 and Spring Boot 3. SPRING FRAMEWORK 6 AND SPRING BOOT 3 We now ship components to support Spring Framework 6 and Spring Boot 3: &lt;dependency&gt; &lt;groupId&gt;org.infinispan&lt;/groupId&gt; &lt;artifactId&gt;infinispan-spring-boot3-starter-embedded&lt;/artifactId&gt; &lt;version&gt;14.0.7.Final&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.infinispan&lt;/groupId&gt; &lt;artifactId&gt;infinispan-spring-boot3-starter-remote&lt;/artifactId&gt; &lt;version&gt;14.0.7.Final&lt;/version&gt; &lt;/dependency&gt; IMPORTANT SIFS FIXES This release also includes very important fixes to the Soft-Index File Store (SIFS), which is our default file-store implementation: if you use it for your persistent caches you should really upgrade ! RELEASE NOTES FEATURE REQUEST * * BUG * * * * * * * * * * * * * * * * * * * * * * * * TASK * * * COMPONENT UPGRADE * * * * * * * * ENHANCEMENT * * * * * * * RELEASE NOTES You can look at the to see what has changed since our latest CR.] Get them from our .]</content><dc:creator>Tristan Tarrant</dc:creator></entry><entry><title type="html">How to use Keycloak Admin REST API</title><link rel="alternate" href="http://www.mastertheboss.com/keycloak/how-to-use-keycloak-admin-rest-api/" /><author><name>F.Marchioni</name></author><id>http://www.mastertheboss.com/keycloak/how-to-use-keycloak-admin-rest-api/</id><updated>2023-03-10T16:42:44Z</updated><content type="html">The Keycloak REST Admin API is a Web service Endpoint that allows you to manage Keycloak programmatically. It provides endpoints for creating, updating, and deleting Keycloak entities such as users, groups, clients, roles, and realms. You can use any programming language that supports HTTP requests to interact with the API. Pre-requisite: If you are new ... The post appeared first on .</content><dc:creator>F.Marchioni</dc:creator></entry><entry><title type="html">What&amp;#8217;s new in Dashbuilder 0.27.0?</title><link rel="alternate" href="https://blog.kie.org/2023/03/whats-new-in-dashbuilder-0-27-0.html" /><author><name>William Siqueira</name></author><id>https://blog.kie.org/2023/03/whats-new-in-dashbuilder-0-27-0.html</id><updated>2023-03-10T11:40:46Z</updated><content type="html">was released yesterday (March 8th, 2023) and it comes with very interesting new features. Let’s explore the new release in this post! NEW SAMPLES SCREEN A new sample screen allows Dashbuilder to render dashboards from a remote or local repository. A sample is a dashboard that users can try without installing on their Dashbuilder. The samples’ screen can be accessed when no dashboard is available. You can see the samples’ screen in action in our , but soon you will hear about it because it will be part of . AUTOCOMPLETE AND SYNTAX CHECK Dashbuilder now supports autocomplete and syntax verification in and ! CONTROLLED CONCURRENT DATASET ACCESS Now dashbuilder unifies HTTP requests when multiple displayers try to access the same dataset. In practice if you have X displayers on a page, it used to result in X HTTP requests, now Dashbuilder makes a single request for all displayers. BUG FIXES Fixed navigation between dashboards: It is possible to have more than one dashboard on the same dashbuilder installation. The navigation between dashboards now refreshes the page, making it easier to navigate between dashboards; Dark mode in components: Victory Charts and Table external components now supports dark mode; Dark mode in VSCode: Dark mode is now fixed in VSCode extension CONCLUSION In this post we shared the new features and improvements in Dashbuilder 0.27.0! Stay tuned for more dashbuilder news. The post appeared first on .</content><dc:creator>William Siqueira</dc:creator></entry><entry><title>JBoss Tools for Eclipse 2023-03M3</title><link rel="alternate" type="text/html" href="https://tools.jboss.org/blog/4.27.0.am1.html" /><category term="release" /><category term="jbosstools" /><category term="jbosscentral" /><author><name>sbouchet</name></author><id>https://tools.jboss.org/blog/4.27.0.am1.html</id><updated>2023-03-11T12:06:25Z</updated><published>2023-03-10T00:00:00Z</published><content type="html">&lt;div&gt;&lt;div id="preamble"&gt; &lt;div class="sectionbody"&gt; &lt;div class="paragraph"&gt; &lt;p&gt;Happy to announce 4.27.0.AM1 (Developer Milestone 1) build for Eclipse 2023-03M3.&lt;/p&gt; &lt;/div&gt; &lt;div class="paragraph"&gt; &lt;p&gt;Downloads available at &lt;a href="https://tools.jboss.org/downloads/jbosstools/2023-03/4.27.0.AM1.html"&gt;JBoss Tools 4.27.0 AM1&lt;/a&gt;.&lt;/p&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; &lt;div class="sect1"&gt; &lt;h2 id="what-is-new"&gt;&lt;a class="anchor" href="#what-is-new"&gt;&lt;/a&gt;What is New?&lt;/h2&gt; &lt;div class="sectionbody"&gt; &lt;div class="paragraph"&gt; &lt;p&gt;Full info is at &lt;a href="https://tools.jboss.org/documentation/whatsnew/jbosstools/4.27.0.AM1.html"&gt;this page&lt;/a&gt;. Some highlights are below.&lt;/p&gt; &lt;/div&gt; &lt;div class="sect2"&gt; &lt;h3 id="general"&gt;&lt;a class="anchor" href="#general"&gt;&lt;/a&gt;General&lt;/h3&gt; &lt;div class="sect3"&gt; &lt;h4 id="components-depreciation"&gt;&lt;a class="anchor" href="#components-depreciation"&gt;&lt;/a&gt;Components Depreciation&lt;/h4&gt; &lt;div class="paragraph"&gt; &lt;p&gt;As previously announced &lt;a href="https://issues.redhat.com/browse/JBIDE-28678"&gt;here&lt;/a&gt;, we’re in the process to remove the Central / update tab from JBossTools in next release. This work can be followed &lt;a href="https://issues.redhat.com/browse/JBIDE-28852"&gt;here&lt;/a&gt;.&lt;/p&gt; &lt;/div&gt; &lt;div class="paragraph"&gt; &lt;p&gt;That means that all the current extra features that can be installed via this tab needs to be available through a new channel. This channel already exists as p2 repo, but using &lt;a href="https://www.eclipse.org/mpc/"&gt;Eclipse Marketplace Client&lt;/a&gt; is more close to what’s existing right now.&lt;/p&gt; &lt;/div&gt; &lt;div class="paragraph"&gt; &lt;p&gt;Most of those additional features are already present in the &lt;a href="https://marketplace.eclipse.org/content/jboss-tools"&gt;Jboss marketplace entry&lt;/a&gt;, so it’s just a matter of use it to install your favorite feature.&lt;/p&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; &lt;div class="sect2"&gt; &lt;h3 id="openshift"&gt;&lt;a class="anchor" href="#openshift"&gt;&lt;/a&gt;OpenShift&lt;/h3&gt; &lt;div class="sect3"&gt; &lt;h4 id="openshift-application-explorer-view-service-creation-support"&gt;&lt;a class="anchor" href="#openshift-application-explorer-view-service-creation-support"&gt;&lt;/a&gt;OpenShift Application Explorer view service creation support&lt;/h4&gt; &lt;div class="paragraph"&gt; &lt;p&gt;The missing create service feature that was available with odo 2.X is now back in this release.&lt;/p&gt; &lt;/div&gt; &lt;div class="paragraph"&gt; &lt;p&gt;&lt;a href="https://tools.jboss.org/documentation/whatsnew/jbosstools/4.21.0.AM1.html#operator-based-services"&gt;See the previous annoucement&lt;/a&gt; on this feature&lt;/p&gt; &lt;/div&gt; &lt;/div&gt; &lt;div class="sect3"&gt; &lt;h4 id="openshift-application-explorer-view-service-binding-support"&gt;&lt;a class="anchor" href="#openshift-application-explorer-view-service-binding-support"&gt;&lt;/a&gt;OpenShift Application Explorer view service binding support&lt;/h4&gt; &lt;div class="paragraph"&gt; &lt;p&gt;Once you have created a service, you can link it to a component using a binding.&lt;/p&gt; &lt;/div&gt; &lt;div class="imageblock"&gt; &lt;div class="content"&gt; &lt;img src="https://tools.jboss.org/documentation/whatsnew/openshift/images/link-service.gif" alt="link service" width="80%" /&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; &lt;div class="sect2"&gt; &lt;h3 id="hibernate-tools"&gt;&lt;a class="anchor" href="#hibernate-tools"&gt;&lt;/a&gt;Hibernate Tools&lt;/h3&gt; &lt;div class="sect3"&gt; &lt;h4 id="runtime-provider-updates"&gt;&lt;a class="anchor" href="#runtime-provider-updates"&gt;&lt;/a&gt;Runtime Provider Updates&lt;/h4&gt; &lt;div class="paragraph"&gt; &lt;p&gt;A new Hibernate 6.2 runtime provider incorporates Hibernate Core version 6.2.0.CR2, Hibernate Ant version 6.2.0.CR2 and Hibernate Tools version 6.2.0.CR2.&lt;/p&gt; &lt;/div&gt; &lt;div class="paragraph"&gt; &lt;p&gt;The Hibernate 6.1 runtime provider now incorporates Hibernate Core version 6.1.7.Final, Hibernate Ant version 6.1.7.Final and Hibernate Tools version 6.1.7.Final.&lt;/p&gt; &lt;/div&gt; &lt;div class="paragraph"&gt; &lt;p&gt;The Hibernate 5.6 runtime provider now incorporates Hibernate Core version 5.6.15.Final and Hibernate Tools version 5.6.15.Final.&lt;/p&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; &lt;div class="sect2"&gt; &lt;h3 id="and-more"&gt;&lt;a class="anchor" href="#and-more"&gt;&lt;/a&gt;And more…​&lt;/h3&gt; &lt;div class="paragraph"&gt; &lt;p&gt;You can find more noteworthy updates in on &lt;a href="https://tools.jboss.org/documentation/whatsnew/jbosstools/4.27.0.AM1.html"&gt;this page&lt;/a&gt;.&lt;/p&gt; &lt;/div&gt; &lt;div class="paragraph"&gt; &lt;p&gt;Enjoy!&lt;/p&gt; &lt;/div&gt; &lt;div class="paragraph"&gt; &lt;p&gt;Stéphane Bouchet&lt;/p&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt;&lt;/div&gt;</content><summary>Happy to announce 4.27.0.AM1 (Developer Milestone 1) build for Eclipse 2023-03M3. Downloads available at JBoss Tools 4.27.0 AM1. What is New? Full info is at this page. Some highlights are below. General Components Depreciation As previously announced here, we’re in the process to remove the Central / update tab from JBossTools in next release. This work can be followed here. That means that all the current extra features that can be installed via this tab needs to be available through a new channel. This channel already exists as p2 repo, but using Eclipse Marketplace Client is more close to what’s existing right now. Most of those additional features...</summary><dc:creator>sbouchet</dc:creator><dc:date>2023-03-10T00:00:00Z</dc:date></entry><entry><title type="html">WildFly Maven Plugin</title><link rel="alternate" href="https://wildfly.org//news/2023/03/09/WildFly-Maven-Plugin/" /><author><name>James R. Perkins</name></author><id>https://wildfly.org//news/2023/03/09/WildFly-Maven-Plugin/</id><updated>2023-03-09T00:00:00Z</updated><content type="html">The has introduced a new dev goal. If you are familiar with the run goal, this goal is very similar. However, it watches for changes to source files. If changes are found, the WAR is rebuilt and redeployed. This new goal is available in version 4.1.0.Beta3 of the wildfly-maven-plugin. USING THE DEV GOAL In its simplest form you enable the plugin like any other maven plugin: &lt;plugin&gt; &lt;groupId&gt;org.wildfly.plugins&lt;/groupId&gt; &lt;artifactId&gt;wildfly-maven-plugin&lt;/artifactId&gt; &lt;version&gt;${version.wildfly-maven-plugin}&lt;/version&gt; &lt;/plugin&gt; Once added to your POM you can simply execute mvn wildfly:dev on your project, and you should be up and running. With no configuration, a full server is provisioned in your projects build directory, e.g. target. Changes to source files are monitored. If required, the compile:compile goal will be invoked. If a resource needs to be copied, the resources:resources goal will be invoked. In some cases the deployment might also need to be redeployed. You can also configure the goal to provision a custom version of WildFly. Below is an example of configuring a cloud server with the cloud-server layer with an H2 database. &lt;plugin&gt; &lt;groupId&gt;org.wildfly.plugins&lt;/groupId&gt; &lt;artifactId&gt;wildfly-maven-plugin&lt;/artifactId&gt; &lt;version&gt;${version.wildfly-maven-plugin}&lt;/version&gt; &lt;configuration&gt; &lt;feature-packs&gt; &lt;feature-pack&gt; &lt;location&gt;org.wildfly:wildfly-galleon-pack:${version.server}&lt;/location&gt; &lt;/feature-pack&gt; &lt;feature-pack&gt; &lt;location&gt;org.wildfly.cloud:wildfly-cloud-galleon-pack:${version.cloud.fp}&lt;/location&gt; &lt;/feature-pack&gt; &lt;/feature-packs&gt; &lt;layers&gt; &lt;layer&gt;cloud-server&lt;/layer&gt; &lt;layer&gt;h2-driver&lt;/layer&gt; &lt;/layers&gt; &lt;/configuration&gt; &lt;/plugin&gt; Full documentation for the goal can be found in the . LIMITATIONS There are currently a couple limitations on this goal. The first is this goal will only work with WAR deployments. The other is changes to the POM file are not watched. If you change the POM you need to kill, CTRL+C, the job and restart the process. CONCLUSION Hopefully this goal is useful for more rapid development. If you’d like to see new features or have questions on how this new goal works, open a on GitHub or in the . If you find a bug feel free to report it in .</content><dc:creator>James R. Perkins</dc:creator></entry><entry><title type="html">Serverless Logic Web Tools support to OpenAPI and AsyncAPI specifications</title><link rel="alternate" href="https://blog.kie.org/2023/03/serverless-logic-web-tools-support-to-openapi-and-asyncapi-specifications.html" /><author><name>Saravana Balaji</name></author><id>https://blog.kie.org/2023/03/serverless-logic-web-tools-support-to-openapi-and-asyncapi-specifications.html</id><updated>2023-03-08T12:47:26Z</updated><content type="html">While editing Serverless Workflow files, you can register OpenAPI endpoints and AsyncAPI channels and pass them as functions. These functions will be available on the Serverless Workflow Editor’s Service Catalog Explorer. This article explains how one can link OpenAPI and AsyncAPI specifications and have their functions available through autocomplete. This feature is available both on Serverless Logic Web Tools and the VS Code extension. On Web tools, the API specifications can be linked via external registries. SERVICE CATALOG IN WEB TOOLS You can access the Serverless Logic Web Tools. Before creating a workflow, you must download and start the Extended Services. STARTING EXTENDED SERVICES Procedure: 1. In the Serverless Logic Web Tools web application, click on the cogwheel (⚙️) in the top-right corner and go to the “KIE Sandbox Extended Services” tab. 2. Select the “Click to setup” option. 3. Select your operating system from the dropdown. 4. Click on the “Download” option highlighted. 5. The kie_sandbox_extended_services_&lt;os&gt;_&lt;version&gt;.&lt;format&gt; file will be downloaded. Follow the instructions provided therein for moving the downloaded app. 6. Click the next button and follow the instructions provided there to open the application. Some of the features in Serverless Logic Web Tools require integration with Red Hat OpenShift Application and Data Services. Now, in the Red Hat OpenShift application and Data Services, let us create a service account. CREATING A SERVICE ACCOUNT You can create or use a service account from your Red Hat OpenShift Application and Data Services console and add the service account to the Serverless Logic Web Tools. Prerequisites * You have access to the Red Hat OpenShift Application and Data Services console. Procedure 1. To create a service account in Red Hat Openshift Application and Data Services, perform the following steps: 1. Go to. 2. Click “Create Service Account.” 3. Enter a service account name in the Short description field of the Create a Service Account window. 2. Click Create. 1. A modal displaying your client ID and client secret appears. 2. Copy and save the client ID and client secret. 3. Check that I have copied the client ID and secret checkboxes, and click Close. 3. If you already have a service account, find your client ID and client secret. 4. In the Serverless Logic Web Tools, click on the cogwheel (⚙️) in the top-right corner and go to the Service Account tab. 5. Enter your client ID and client secret in the respective fields. 6. Click Apply. 7. The content in the Service Account tab updates and displays Your Service Account information as a set message. CREATING A SERVICE REGISTRY You can create or use a Service Registry instance from your Red Hat OpenShift Application and Data Services console and add the Service Registry to Serverless Logic Web Tools. Prerequisites * You have access to the Red Hat OpenShift Application and Data Services console. * You have created a service account. Procedure: 1. To create a Service Registry instance in Red Hat Openshift Application and Data Services console, perform the following steps: 1. Go to the. 2. Click the “Create Service Registry Instance” button. 2. Enter a Service Registry instance name in the Create a Service Registry Instance window and click Create. 1. The list of Service Registry instances updates with your instance. 2. Find the Service Registry instance you created in the list and click on it. 3. Go to the Settings tab and click on “Grant access.” 4. In the drop-down, select the service account you created in the previous procedure. 5. Select a role for your service account. 6. Click Save. 7. Click on the menu in the top-right corner of the screen. 3. Click Connection. 1. A drawer opens, containing the required connection and authentication information. 2. Copy the value of the Core Registry API. 4. If you already have a Service Registry, find the value of the Core Registry API of your Service Registry. 5. In the Serverless Logic Web Tools web application, click on the cogwheel (⚙️) in the top-right corner and go to the Service Registry tab. 6. Enter a name for your registry. You can enter the same name that you used while creating the Service Registry instance. 7. Enter the value of the Core Registry API and click Apply. 8. The Service Registry tab’s content updates and displays your Service Registry information in a predefined message. UPLOAD THE API SPECIFICATION Once you have the Service account and Service registry created, you are set to upload the API specification file, which could be an OpenAPI or an AsyncAPI. Procedure: 1. On the OpenShift Application Console, go to the Service Registry instance that you created. 2. Click the Upload Artifact button, and a modal window opens. 3. On the modal, enter the group and artifacts ID, which are mandatory fields for usage in the Serverless Workflow Web Tools. 4. You can choose type from the dropdown menu, or you can leave the default option Auto-Detect alone. 5. In the artifact section, you can browse or drag and drop the API specification file, or you can also copy-paste the content directly into the text area. 6. Finally, click “upload.” ACCESSING THE SERVICE CATALOG ON WEB TOOLS When you have all the prerequisites ready, it is possible to create a workflow and try the Service Catalog feature. You can create a new workflow by clicking the JSON or YAML button on the home page. An untitled document will be opened. You can click on the Create a Serverless Workflow option to generate a basic template workflow. Just above the functions property, there will be an Add Function option. When you click that option, a side widget containing a list of function suggestions will appear. These functions are extracted from the API specification files that you uploaded on the Red Hat OpenShift Application Console. When you select a particular function, it will automatically generate auto-filled name, operation, and type properties. That is all for now, soon we will have an article demonstrating the Service Catalog in the Serverless Workflow Editor VS Code extension. Stay tuned! The post appeared first on .</content><dc:creator>Saravana Balaji</dc:creator></entry><entry><title>Our advice for configuring Knative Broker for Apache Kafka</title><link rel="alternate" href="https://developers.redhat.com/articles/2023/03/08/configuring-knative-broker-apache-kafka" /><author><name>Matthias Wessendorf</name></author><id>04600ac2-35c4-4460-a212-3719a3c5c1e6</id><updated>2023-03-08T07:00:00Z</updated><published>2023-03-08T07:00:00Z</published><summary type="html">&lt;p&gt;In this article, you will learn how to set up the &lt;a href="https://knative.dev/docs/eventing/brokers/broker-types/kafka-broker/"&gt;Knative Broker implementation for Apache Kafka&lt;/a&gt; in a production environment. Recently, the Kafka implementation has been announced as &lt;a href="https://developers.redhat.com/articles/2022/11/15/knative-broker-enhances-kafka-openshift-serverless"&gt;General Availablitly&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;One common question for production systems is always the need for an optimal configuration based on the given environment. This article gives recommendations on how to pick a good default configuration for the Knative Broker implementation for Apache Kafka.&lt;/p&gt; &lt;p&gt;The article is based on a small yet production-ready setup of Apache Kafka and Apache Zookeeper, each system consisting of three nodes. You can find a reference Kafka configuration based on &lt;a href="https://strimzi.io/"&gt;Strimzi.io&lt;/a&gt; on the &lt;a href="https://github.com/strimzi/strimzi-kafka-operator/blob/0.32.0/examples/kafka/kafka-persistent.yaml"&gt;Strimzi GitHub page&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Note:&lt;/strong&gt; This article is not giving recommendations for the configuration of the Kubernetes cluster.&lt;/p&gt; &lt;h2&gt;Setting up the Knative Broker&lt;/h2&gt; &lt;p&gt;Each broker object uses a Kafka topic for the storage of incoming CloudEvents. The recommendation to start with is as follows:&lt;/p&gt; &lt;ul&gt;&lt;li&gt;Partitions: 10&lt;/li&gt; &lt;li&gt;Replication Factor: 3&lt;/li&gt; &lt;/ul&gt;&lt;p&gt;Taking this configuration gives you a fault-tolerant, highly-available, and yet scalable basis for your project. Topics are partitioned, meaning they are spread over a number of buckets located on different Kafka brokers. To make your data fault tolerant and highly available, every topic can be replicated, even across geo-regions or datacenters. You can find more detailed information in the &lt;a href="https://kafka.apache.org/intro"&gt;Apache Kafka documentation&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Note:&lt;/strong&gt; Configuration aspects such as topic partitions or replication factor directly relate to the actual sizing of the cluster. For instance, if your cluster consists of three nodes, you cannot set the replication factor to a higher number, as it directly relates to the available nodes of the Apache Kafkacluster. You can find details about implications in the &lt;a href="https://strimzi.io/documentation/"&gt;Strimzi documentation&lt;/a&gt;.&lt;/p&gt; &lt;h2&gt;An example Knative Broker configuration&lt;/h2&gt; &lt;p&gt;Let us take a look at a possible configuration of a Knative Broker for Apache Kafka with this defined cluster in mind:&lt;/p&gt; &lt;pre&gt; &lt;code&gt;apiVersion: v1 kind: ConfigMap metadata: name: &lt;broker-name&gt;-config data: bootstrap.servers: &lt;url&gt; auth.secret.ref.name: &lt;optional-secret-name&gt; default.topic.partitions: "10" default.topic.replication.factor: "3" --- apiVersion: eventing.knative.dev/v1 kind: Broker metadata: annotations: eventing.knative.dev/broker.class: Kafka name: &lt;broker-name&gt; spec: config: apiVersion: v1 kind: ConfigMap name: &lt;broker-name&gt;-config delivery: retry: 12 backoffPolicy: exponential backoffDelay: PT0.5S deadLetterSink: ref: apiVersion: eventing.knative.dev/v1 kind: Broker name: &lt;dead-letter-sink-broker-name&gt;&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;We see two manifests defined:&lt;/p&gt; &lt;ul&gt;&lt;li&gt;A &lt;code&gt;ConfigMap&lt;/code&gt; resource&lt;/li&gt; &lt;li&gt;A &lt;code&gt;Broker&lt;/code&gt; resource.&lt;/li&gt; &lt;/ul&gt;&lt;p&gt;The &lt;code&gt;ConfigMap&lt;/code&gt; defines the URL to the Apache Kafka cluster and references a secret for TLS/SASL &lt;a href="https://knative.dev/docs/eventing/brokers/broker-types/kafka-broker/#security"&gt;security support&lt;/a&gt;. The manifest also sets the partitions and replication factor for the Kafka topic, internally used for the &lt;code&gt;Broker&lt;/code&gt; object, to store incoming CloudEvents as Kafka records. The &lt;code&gt;Broker&lt;/code&gt; uses the &lt;code&gt;eventing.knative.dev/broker.class&lt;/code&gt; indicating the Kafka-based Knative Broker implementation should be used. On the &lt;code&gt;spec&lt;/code&gt;, it references the &lt;code&gt;ConfigMap&lt;/code&gt;, as well configuration on the broker's event delivery.&lt;/p&gt; &lt;h3&gt;Broker event delivery guarantees and retries&lt;/h3&gt; &lt;p&gt;Knative provides various configuration parameters to control the delivery of events in case of failure. This configuration is able to define a number of retry attempts, a backoff delay, and a backoff policy (linear or exponential). If the event delivery is not successful, the event is delivered to the dead letter sink, if present. The &lt;code&gt;delivery&lt;/code&gt; spec object is global for the given broker object and can be overridden on a per &lt;code&gt;Trigger &lt;/code&gt;basis, if needed.&lt;/p&gt; &lt;p&gt;The following resources offer more details on the &lt;code&gt;delivery&lt;/code&gt; spec:&lt;/p&gt; &lt;ul&gt;&lt;li&gt;&lt;a href="https://github.com/knative/specs/blob/main/specs/eventing/data-plane.md"&gt;Knative Eventing Data Plane Contract&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://knative.dev/docs/eventing/event-delivery/#configuring-broker-event-delivery"&gt;Broker event delivery&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt;&lt;h3&gt;The Knative Broker as a dead letter sink approach&lt;/h3&gt; &lt;p&gt;The &lt;code&gt;deadLetterSink&lt;/code&gt; object can be any &lt;a href="https://knative.dev/docs/concepts/duck-typing/#addressable"&gt;&lt;code&gt;Addressable&lt;/code&gt;&lt;/a&gt; object that conforms to the Knative Eventing sink contract, such as a Knative Service, a Kubernetes Service, or a URI. However, this example is configuring a &lt;code&gt;deadLetterSink&lt;/code&gt;, with the different broker object. The benefit of this highly recommended approach is that all undelivered messages are sent to a Knative broker object and consumed from there, using the standard Knative Broker and Trigger APIs.&lt;/p&gt; &lt;h2&gt;An example trigger configuration&lt;/h2&gt; &lt;p&gt;The triggers are used for the egress out of the broker to consuming services. Triggers are executed on the referenced broker object. The following is an example of a trigger configuration:&lt;/p&gt; &lt;pre&gt; &lt;code&gt;apiVersion: eventing.knative.dev/v1 kind: Trigger metadata: name: &lt;trigger-name&gt; annotations: kafka.eventing.knative.dev/delivery.order: &lt;delivery-order&gt; spec: broker: &lt;broker-name&gt; filter: attributes: type: &lt;cloud-event-type&gt; &lt;ce-extension&gt;: &lt;ce-extension-value&gt; subscriber: ref: apiVersion: v1 kind: Service name: &lt;receiver-name&gt;&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;We see a trigger configuration for a specific &lt;code&gt;broker&lt;/code&gt; object, which filters the available CloudEvents, their attributes, and custom CloudEvent extension attributes. Matching events are routed to the &lt;code&gt;subscriber&lt;/code&gt;, which can be any &lt;a href="https://knative.dev/docs/concepts/duck-typing/#addressable"&gt;&lt;code&gt;Addressable&lt;/code&gt;&lt;/a&gt; object that conforms to the Knative Eventing sink contract, as defined above.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Note:&lt;/strong&gt; We recommend applying filters on triggers for CloudEvent attributes and extensions. If no &lt;code&gt;filter&lt;/code&gt; is provided, all occurring CloudEvents are routed to the referenced &lt;code&gt;subscriber&lt;/code&gt;.&lt;/p&gt; &lt;h3&gt;Message delivery order&lt;/h3&gt; &lt;p&gt;When dispatching events to the &lt;code&gt;subscriber&lt;/code&gt;, the Knative Kafka Broker can be configured to support different delivery ordering guarantees by using the &lt;code&gt;kafka.eventing.knative.dev/delivery.order&lt;/code&gt; annotation on every &lt;code&gt;Trigger&lt;/code&gt; object.&lt;/p&gt; &lt;p&gt;The supported consumer delivery guarantees are:&lt;/p&gt; &lt;ul&gt;&lt;li dir="ltr"&gt;&lt;code&gt;unordered&lt;/code&gt;: An unordered consumer is a non-blocking consumer that delivers messages unordered while preserving proper offset management. This is useful when there is a high demand for parallel consumption and no need for explicit ordering. One example could be the processing of click analytics.&lt;/li&gt; &lt;li dir="ltr"&gt;&lt;code&gt;ordered&lt;/code&gt;: An ordered consumer is a per-partition blocking consumer that waits for a successful response from the CloudEvent subscriber before it delivers the next message of the partition. This is useful when there is a need for more strict ordering or if there is a relationship or grouping between events. One example could be the processing of customer orders.&lt;/li&gt; &lt;/ul&gt;&lt;p&gt;&lt;strong&gt;Note:&lt;/strong&gt; The default ordering guarantee is &lt;code&gt;unordered&lt;/code&gt;.&lt;/p&gt; &lt;h3&gt;Trigger delivery guarantees and retries&lt;/h3&gt; &lt;p&gt;The global broker delivery configuration can be overridden on a per-trigger basis using the &lt;code&gt;delivery&lt;/code&gt; on the &lt;code&gt;Trigger spec&lt;/code&gt;. The behavior is the same as defined for the Broker event delivery guarantees and retries.&lt;/p&gt; &lt;h2&gt;Our advice for Knative Broker configuration&lt;/h2&gt; &lt;p&gt;In this article, we demonstrated how to set up the &lt;a href="https://knative.dev/docs/eventing/brokers/broker-types/kafka-broker/"&gt;Knative Broker implementation for Apache Kafka&lt;/a&gt; in a production environment. We provided recommendations for picking a good default configuration for the Knative Broker implementation for Apache Kafka. Feel free to comment below if you have questions. We welcome your feedback.&lt;/p&gt; The post &lt;a href="https://developers.redhat.com/articles/2023/03/08/configuring-knative-broker-apache-kafka" title="Our advice for configuring Knative Broker for Apache Kafka"&gt;Our advice for configuring Knative Broker for Apache Kafka&lt;/a&gt; appeared first on &lt;a href="https://developers.redhat.com/blog" title="Red Hat Developer"&gt;Red Hat Developer&lt;/a&gt;. &lt;br /&gt;&lt;br /&gt;</summary><dc:creator>Matthias Wessendorf</dc:creator><dc:date>2023-03-08T07:00:00Z</dc:date></entry></feed>
